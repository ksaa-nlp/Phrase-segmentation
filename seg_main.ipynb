{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8eGMhjgdZNdg"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPdxhC8eCU0j"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LuM7MqP_ZNdn"
      },
      "outputs": [],
      "source": [
        "data = pd.read_excel(\"full_dataset.xlsx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zJuJQpGZNdp"
      },
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hd-kcDjp09Ka"
      },
      "source": [
        "#### Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "il1a88IwZNdt"
      },
      "outputs": [],
      "source": [
        "data.rId.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5YEyPk41aV2"
      },
      "outputs": [],
      "source": [
        "#max no. of sentences per text\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.countplot(data=data, x='rId')\n",
        "plt.xticks([])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGibzs632La2"
      },
      "outputs": [],
      "source": [
        "df_infos = pd.DataFrame(data.rId.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZizjAGe-2TyV"
      },
      "outputs": [],
      "source": [
        "df_infos['rId'].mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoQMG2rk0_dV"
      },
      "source": [
        "#### Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bN-eIFgV2bRX"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "cxkde0kO41Qu"
      },
      "outputs": [],
      "source": [
        "data_sent = data[['_id', 'sentence', 'rId']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzdVTKOG43qQ"
      },
      "outputs": [],
      "source": [
        "data_sent['sentence'].isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "d2qQA7ai5Rna"
      },
      "outputs": [],
      "source": [
        "data_sent = data_sent.dropna(subset=['sentence'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FInalZbt5gEw"
      },
      "outputs": [],
      "source": [
        "data_sent.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jwvUVPE5kYh"
      },
      "source": [
        "#### Tokenisation & encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeR8vqAF2zKa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "model_name = \"aubmindlab/bert-base-arabertv02\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "sentences = data_sent.sentence.tolist()\n",
        "\n",
        "def tokenize(sent) :\n",
        "  sent_tokeinzed = tokenizer.tokenize(sent)\n",
        "  sent_iob = ['1'] + ['0'] * (len(sent_tokeinzed)-1)\n",
        "  return  sent_tokeinzed, sent_iob\n",
        "\n",
        "data_sent['sent_tokenized'], data_sent['sent_iob'] = zip(*data_sent['sentence'].apply(tokenize))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TvEf1RMovN-"
      },
      "outputs": [],
      "source": [
        "all_pargraphs = data_sent.rId.unique().tolist()\n",
        "len(all_pargraphs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preprocessing (didn't improve the results)"
      ],
      "metadata": {
        "id": "vKZ0Azfuk9yp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyarabic"
      ],
      "metadata": {
        "id": "xWQuJJl0lDyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyarabic.araby as araby\n",
        "\n",
        "def clean_text(text):\n",
        "    ## remove extra whitespace\n",
        "    text = re.sub('\\s+', ' ', str(text))\n",
        "    ## Remove Tashkeel\n",
        "    text = araby.strip_diacritics(str(text))\n",
        "    ## remove extra whitespace\n",
        "    text = re.sub('\\s+', ' ', str(text))\n",
        "    ## Remove numbers\n",
        "    text = re.sub(\"\\d+\", \" \", text)\n",
        "    ## Remove Non-Arabic Letters\n",
        "    text = re.sub('[A-Za-z]+',' ',text)\n",
        "    return text\n",
        "\n",
        "data_sent['sentence']=data_sent['sentence'].apply(clean_text)"
      ],
      "metadata": {
        "id": "rbZZPHUKk5CR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuIzFwT5qJ2_"
      },
      "source": [
        "#### Embedding & concatenating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7V-xyRIsppT"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxgL_OvFtz0t"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "   print(\"Training on GPU\")\n",
        "   device = torch.device(\"cuda:0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3wqbr59bS6M"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, AutoTokenizer, AutoModel\n",
        "\n",
        "model_name = \"aubmindlab/bert-base-arabertv02\"\n",
        "model = AutoModel.from_pretrained(model_name).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Generate 10K paragraphs"
      ],
      "metadata": {
        "id": "uVEqehFXkWds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# concatenation\n",
        "\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "ids = []\n",
        "sentences_concat = []\n",
        "sentences_token_concat = []\n",
        "iob_encoding_concat = []\n",
        "\n",
        "for paragraph_id in tqdm(data_sent.rId.unique()):\n",
        "    data_paragraph = data_sent[data_sent['rId'] == paragraph_id]\n",
        "    phrases = data_paragraph['sentence'].tolist()\n",
        "\n",
        "    while phrases:\n",
        "        num = random.choice([3, 4, 5, 6, 7, 8, 9])\n",
        "        df_paragraph = data_paragraph.iloc[:num]\n",
        "\n",
        "        phrase_concat = ' '.join(df_paragraph['sentence'])\n",
        "        phrase_token_list = sum(df_paragraph['sent_tokenized'].tolist(), [])\n",
        "        iob_list = sum(df_paragraph['sent_iob'].tolist(), [])\n",
        "        id_ = df_paragraph['rId'].iloc[0]\n",
        "        inputs = tokenizer(phrase_concat, return_tensors=\"pt\", is_split_into_words=True).to(device)\n",
        "\n",
        "        # Check if the length exceeds 512\n",
        "        while len(inputs['input_ids'][0]) > 512:\n",
        "            num -= 1  # reduce the threshold\n",
        "            if num < 1:  # to avoid an infinite loop\n",
        "                break\n",
        "            df_paragraph = data_paragraph.iloc[:num]\n",
        "            phrase_concat = ' '.join(df_paragraph['sentence'])\n",
        "\n",
        "            inputs = tokenizer(phrase_concat, return_tensors=\"pt\", is_split_into_words=True,truncation=True,max_length=512).to(device)\n",
        "\n",
        "        outputs = model(**inputs)\n",
        "        sentences_concat.append(phrase_concat)\n",
        "        sentences_token_concat.append(phrase_token_list)\n",
        "        iob_encoding_concat.append(iob_list)\n",
        "        ids.append(id_)\n",
        "\n",
        "        data_paragraph = data_paragraph.iloc[num:]\n",
        "        phrases = data_paragraph['sentence'].tolist()"
      ],
      "metadata": {
        "id": "5_47pywcloYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Generate 400K paragraphs"
      ],
      "metadata": {
        "id": "ryegM1zNlgiW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-Z2vcAhQujJ"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "ids = []\n",
        "sentences_concat = []\n",
        "sentences_token_concat = []\n",
        "\n",
        "for paragraph_id in tqdm(data_sent.rId.unique()):\n",
        "    data_paragraph = data_sent[data_sent['rId'] == paragraph_id]\n",
        "    phrases = data_paragraph['sentence'].tolist()\n",
        "    while (len(phrases) >= 3):\n",
        "        for num in range(3, 10):\n",
        "            df_paragraph = data_paragraph.iloc[:num]\n",
        "\n",
        "            phrase_concat = ' '.join(df_paragraph['sentence'])\n",
        "            phrase_token_list = sum(df_paragraph['sent_tokenized'].tolist(), [])\n",
        "            iob_list = sum(df_paragraph['sent_iob'].tolist(), [])\n",
        "            id_ = df_paragraph['rId'].iloc[0]\n",
        "\n",
        "            if len(iob_list) > 510:\n",
        "                continue\n",
        "\n",
        "            sentences_concat.append(phrase_concat)\n",
        "            sentences_token_concat.append(phrase_token_list)\n",
        "            iob_encoding_concat.append(iob_list)\n",
        "            ids.append(id_)\n",
        "\n",
        "        data_paragraph = data_paragraph.iloc[1:]\n",
        "        phrases = data_paragraph['sentence'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXrUaikpQzN-"
      },
      "outputs": [],
      "source": [
        "data_sent_concat = pd.DataFrame()\n",
        "data_sent_concat['sent_concat'] = sentences_concat\n",
        "data_sent_concat['sent_token_concat'] = sentences_token_concat\n",
        "data_sent_concat['iob_concat'] = iob_encoding_concat\n",
        "data_sent_concat['rId'] = ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q32BkD8ESlWG"
      },
      "outputs": [],
      "source": [
        "data_sent_concat.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwX8p_pSyLlj"
      },
      "source": [
        "#### Train Bi-LSTM (Pytorch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vLP-EvlBXIee"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "data_sent_concat['iob_concat'] = data_sent_concat['iob_concat'].apply(lambda x: list(map(int, ast.literal_eval(x))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Qwi_P43nE93e"
      },
      "outputs": [],
      "source": [
        "data_sent_concat[\"sent_concat\"] = data_sent_concat[\"sent_concat\"].astype(str)\n",
        "X_padded = data_sent_concat['sent_concat'].to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLZJiUqdPx-J"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "model_name=\"aubmindlab/bert-base-arabertv02\"\n",
        "bert = AutoModel.from_pretrained(model_name)\n",
        "maxlen = bert.config.max_position_embeddings - 2 # 2 for the special tokens [CLS] and [SEP]\n",
        "\n",
        "#maxlen = 510"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNH-ep6WUeSq"
      },
      "outputs": [],
      "source": [
        "maxlen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJCIJkO9YLYH"
      },
      "outputs": [],
      "source": [
        "#data_sent_concat.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "FyO_LfA5eZAZ"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "data_sent_concat['iob_concat'] = data_sent_concat['iob_concat'].apply(lambda x: list(map(int, ast.literal_eval(x))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "zSqKuplLPhfW"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "y_padded = pad_sequences(data_sent_concat['iob_concat'], padding='post', dtype='int32', maxlen=maxlen)\n",
        "y_padded = np.expand_dims(y_padded, axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ffpBiQ6aWOG"
      },
      "outputs": [],
      "source": [
        "y_padded.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Pjoxhn6aFgmf"
      },
      "outputs": [],
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X_padded, y_padded, test_size=0.2, random_state=42)\n",
        "\n",
        "len_x = len(X_padded)\n",
        "num = int(len(X_padded)*0.9)\n",
        "X_train = X_padded[:num]\n",
        "y_train = y_padded[:num]\n",
        "X_test = X_padded[num:]\n",
        "y_test = y_padded[num:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwXas0oEQU0n"
      },
      "outputs": [],
      "source": [
        "len(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Xo9J-pqQ5u5"
      },
      "outputs": [],
      "source": [
        "len(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKMADJd1IQzp"
      },
      "outputs": [],
      "source": [
        "X_padded[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "w1Rf9-8ZEXEq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "# from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"aubmindlab/bert-base-arabertv02\")\n",
        "tokenizer_kwargs = {\n",
        "    \"return_tensors\": \"pt\",\n",
        "    \"padding\": \"max_length\",\n",
        "    \"truncation\": True,\n",
        "    \"max_length\": 512\n",
        "}\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, X, y, tokenizer, tokenizer_kwargs):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.tokenizer = tokenizer\n",
        "        self.tokenizer_kwargs = tokenizer_kwargs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inputs = self.tokenizer(self.X[idx], **self.tokenizer_kwargs)\n",
        "        return inputs, [self.y[idx]]\n",
        "\n",
        "    def __getitems__(self, idxs):\n",
        "        inputs = self.tokenizer([self.X[idx] for idx in idxs], **self.tokenizer_kwargs)\n",
        "        return [(inputs, self.y[idxs])]\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    X = {\n",
        "        \"input_ids\": [],\n",
        "        \"attention_mask\": [],\n",
        "        \"token_type_ids\": []\n",
        "    }\n",
        "    y = []\n",
        "    for x, yi in batch:\n",
        "        X[\"input_ids\"].append(x[\"input_ids\"])\n",
        "        X[\"attention_mask\"].append(x[\"attention_mask\"])\n",
        "        X[\"token_type_ids\"].append(x[\"token_type_ids\"])\n",
        "        y.append(yi)\n",
        "    X[\"input_ids\"] = torch.cat(X[\"input_ids\"], dim=0)\n",
        "    X[\"attention_mask\"] = torch.cat(X[\"attention_mask\"], dim=0)\n",
        "    X[\"token_type_ids\"] = torch.cat(X[\"token_type_ids\"], dim=0)\n",
        "    y = torch.cat(y, dim=0)\n",
        "    return X, y\n",
        "\n",
        "# Convert data to tensors\n",
        "# X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "# X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "# Create DataLoaders\n",
        "# train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "# test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "train_dataset = CustomDataset(X_train, y_train_tensor, tokenizer, tokenizer_kwargs)\n",
        "test_dataset = CustomDataset(X_test, y_test_tensor, tokenizer, tokenizer_kwargs)\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, collate_fn=collate_fn) #batch_size=16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Tfp2WH_EEXJN"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "from transformers import BertTokenizer, AutoTokenizer, AutoModel\n",
        "\n",
        "\n",
        "\n",
        "class BiLSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, model_name=\"aubmindlab/bert-base-arabertv02\"):\n",
        "        super(BiLSTMModel, self).__init__()\n",
        "        self.bert = AutoModel.from_pretrained(model_name)\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(2 * hidden_dim, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        with torch.no_grad():\n",
        "            outputs = self.bert(**x)\n",
        "            x = outputs.last_hidden_state[:, 1:-1, :]  # embeddings\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        output = self.fc(lstm_out)\n",
        "        return self.sigmoid(output)\n",
        "\n",
        "\n",
        "# input_dim = X_train.shape[2]  # Assuming that X_train has the form (batch_size, seq_length, input_dim)\n",
        "from transformers import AutoModel\n",
        "\n",
        "model_name=\"aubmindlab/bert-base-arabertv02\"\n",
        "bert = AutoModel.from_pretrained(model_name)\n",
        "input_dim = bert.config.hidden_size\n",
        "\n",
        "\n",
        "#input_dim = 768\n",
        "hidden_dim = 32\n",
        "lstm_model = BiLSTMModel(input_dim, hidden_dim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WxLMkqMg2EP"
      },
      "outputs": [],
      "source": [
        "input_dim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QVUgp4og-Rh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "   print(\"Training on GPU\")\n",
        "   device = torch.device(\"cuda:0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WN9UF0DdEXOk"
      },
      "outputs": [],
      "source": [
        "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "lstm_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDPjYm-pEXRe"
      },
      "outputs": [],
      "source": [
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    lstm_model.train()\n",
        "    for batch_X, batch_y in tqdm(train_loader):\n",
        "        # batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "        batch_X = {k: v.to(device) for k, v in batch_X.items()}\n",
        "        batch_y = batch_y.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = lstm_model(batch_X).squeeze()\n",
        "        loss = criterion(outputs, batch_y.squeeze())\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGgrXbp4ME3G"
      },
      "outputs": [],
      "source": [
        "# Save model:\n",
        "torch.save(lstm_model, '/content/gdrive/MyDrive/model_lstm.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NatjS1Uew-J6"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "#gc.collect()\n",
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:5\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUz9jGNvCkgd"
      },
      "outputs": [],
      "source": [
        "lstm_model = torch.load('/content/gdrive/MyDrive/model_lstm.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwfvnSgFEbS8"
      },
      "outputs": [],
      "source": [
        "lstm_model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-j1TyHMiLV0k"
      },
      "source": [
        "### Splitting score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyiqds8FZv0A"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "all_splits = []\n",
        "all_matches = []\n",
        "\n",
        "for index in tqdm(range(len(X_test))) :\n",
        "  single_example = X_test[index]\n",
        "  true_label = y_test_tensor[index]\n",
        "\n",
        "  single_example = tokenizer(single_example, **tokenizer_kwargs)\n",
        "\n",
        "  lstm_model.eval()\n",
        "\n",
        "  single_example = {k: v.to(device) for k, v in single_example.items()}\n",
        "\n",
        "  with torch.no_grad():\n",
        "      output = lstm_model(single_example)\n",
        "      prediction = (output > 0.5).float()  # Binarize the prediction\n",
        "\n",
        "  splits = np.sum(true_label.numpy()) - 1\n",
        "\n",
        "  # Ignore first element\n",
        "  y_true = true_label.numpy()[1:]\n",
        "  y_pred = prediction.detach().cpu().numpy()[0][1:]\n",
        "\n",
        "# Calculate the number of matches of 1\n",
        "  matches = np.sum((y_true == 1) & (y_pred == 1))\n",
        "\n",
        "  all_splits.append(splits)\n",
        "  all_matches.append(matches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwRRsfD4DQzL"
      },
      "outputs": [],
      "source": [
        "sum(all_matches)/sum(all_splits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shaCYKZowCnS"
      },
      "source": [
        "## Inference\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0fGdSdT52D9"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojwKknczURUp"
      },
      "source": [
        "#### Import some test phrases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z84MF2R4wPuQ"
      },
      "outputs": [],
      "source": [
        "# import data\n",
        "import pandas as pd\n",
        "data = pd.read_excel(\"full_dataset.xlsx\")\n",
        "data_sent = data[['_id', 'sentence', 'rId']]\n",
        "data_sent = data_sent.dropna(subset=['sentence'])\n",
        "data_sent.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrbNQ0K0gDku"
      },
      "outputs": [],
      "source": [
        "# GPU:\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "   print(\"Training on GPU\")\n",
        "   device = torch.device(\"cuda:0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2drSAvm0Yl1B"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "def split_sent(sent) :\n",
        "  # Encode:\n",
        "  inputs = tokenizer(sent, **tokenizer_kwargs).to(device)\n",
        "\n",
        "  #Predict lstm:\n",
        "  with torch.no_grad():\n",
        "      output = lstm_model(inputs)\n",
        "      prediction = (output > 0.5).float()\n",
        "\n",
        "\n",
        "  # Split and decode each segment:\n",
        "  indices = (prediction == 1).nonzero(as_tuple=True)[1].to(device)\n",
        "\n",
        "  indices = torch.cat((indices, torch.tensor([inputs[\"input_ids\"].shape[0]]).to(device)))\n",
        "\n",
        "  segments = []\n",
        "  for i in range(len(indices) - 1):\n",
        "      start_index = indices[i]\n",
        "      end_index = indices[i+1]\n",
        "      segment_ids = inputs[\"input_ids\"][0][start_index:end_index]\n",
        "      segments.append(tokenizer.decode(segment_ids, skip_special_tokens=True))\n",
        "\n",
        "  return(segments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gs2e4_rdTOIf"
      },
      "outputs": [],
      "source": [
        "def split_sent(sent) :\n",
        "  # Encode:\n",
        "  inputs = tokenizer(sent, return_tensors=\"pt\", is_split_into_words=True).to(device)\n",
        "  outputs = bert_model(**inputs)\n",
        "  embeddings = outputs.last_hidden_state[:, 1:-1, :].detach().cpu().numpy()\n",
        "\n",
        "  # Padding:\n",
        "  truncated = embeddings[:maxlen]\n",
        "  y_padded = np.zeros((maxlen, len(embeddings[0][0])))\n",
        "  y_padded[:len(truncated[0])] = truncated[0]\n",
        "  y_padded = torch.tensor(y_padded, dtype=torch.float32).to(device)\n",
        "\n",
        "  #Predict lstm:\n",
        "  with torch.no_grad():\n",
        "      output = lstm_model(y_padded)\n",
        "      prediction = (output > 0.5).float()\n",
        "\n",
        "\n",
        "  # Split and decode each segment:\n",
        "  indices = (prediction == 1).nonzero(as_tuple=True)[0].to(device)\n",
        "\n",
        "  indices = torch.cat((indices, torch.tensor([inputs[\"input_ids\"].shape[1]]).to(device)))\n",
        "\n",
        "  segments = []\n",
        "  for i in range(len(indices) - 1):\n",
        "      start_index = indices[i]\n",
        "      end_index = indices[i+1]\n",
        "      segment_ids = inputs[\"input_ids\"][0][start_index:end_index]\n",
        "      segments.append(tokenizer.decode(segment_ids, skip_special_tokens=True))\n",
        "\n",
        "  return(segments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QToPuPQVOsg1",
        "outputId": "7d2181ea-22e1-4384-d199-801dd32c3b79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "وتسويق المؤثرين هو شكل من أشكال التعاون الذي يحدث عندما تشترك العلامات التجارية مع المؤثرين للترويج لمنتج أو خدمة أو حملة. ويعمل هذا النوع من التسويق بشكل فعال؛ لأن المؤثرين يسيطرون على جمهورك المستهدف؛ حيث إن متابعيهم يثقون بهم بالفعل ويتطلعون إليهم للحصول على توصيات. وبمجرد العثور على المؤثر الذي يشارك المحتوى ذا الصلة ويبدو أنه مناسب لعلامتك التجارية يجب عليك بعد ذلك إقناعه بالعمل معك. وتشير التقديرات الحالية إلى أن سوق تسويق المؤثرين قد يصل إلى بين 5 و10 مليارات دولار بنهاية عام 2020. اقرأ أيضًا: 3 قواعد لتسويق مشروع ناجح.. كيف ترضي عملاءك؟ الفرق بين التسويق الإلكتروني وتسويق المؤثرين يمكننا تلخيص الفرق بين التسويق الإلكتروني وتسويق المؤثرين كما يلي: التسويق الإلكتروني أعم التسويق الإلكتروني جهد عام وشاق، فهو يشمل التسويق عبر محركات البحث، وعبر البريد الإلكتروني، ومن خلال وسائل التواصل الاجتماعي، بل عبر المؤثرين أنفسهم.\n"
          ]
        }
      ],
      "source": [
        "test_seq = data_sent_concat['sent_concat'][0]\n",
        "print(test_seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fm-Mwwq6PANH"
      },
      "outputs": [],
      "source": [
        "# Predicted phrases:\n",
        "split_sent(test_seq)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "HoTTkE2mRzKJ"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}